{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_path = 'peptide-QML'\n",
    "# initial_path = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random\n",
    "sys.path.append(initial_path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from my_code import helper_classes as c\n",
    "# from my_code import pytorch_model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_left(time_start, epoch, i, len_x):\n",
    "    time_left = (time.time() - time_start) * (100*len_x//32 / (epoch*len_x//32 + i + 1) - 1)\n",
    "    hours = int(time_left // 3600)\n",
    "    minutes = int((time_left - hours * 3600) // 60)\n",
    "    seconds = int(time_left - hours * 3600 - minutes * 60)\n",
    "    return hours, minutes, seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=25.572827,                                                                                           \n",
      "Epoch 1, loss=25.516971,                                                                                           \n",
      "Epoch 2, loss=14647723565.375923,                                                                                           \n",
      "Epoch 3, loss=44532.028903,                                                                                           \n",
      "Epoch 4, loss=344.018009,                                                                                           \n",
      "Epoch 5, loss=1260.876336,                                                                                           \n",
      "Epoch 6, loss=142.488106,                                                                                           \n",
      "Epoch 7, loss=164029517.726809,                                                                                           \n",
      "Epoch 8, loss=nan,                                                                                                                  \n",
      "Epoch 9, loss=nan,                                                                                           \n",
      "Epoch 10, loss=nan,                                                                                           \n",
      "Epoch 11, loss=nan,                                                                                           \n",
      "Epoch 12, loss=nan,                                                                                           \n",
      "Epoch 13, loss=nan,                                                                                           \n",
      "Epoch 14, batch 7152/12430, loss=nan, time left=8h 49m 56s                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f713f19bec31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "\n",
    "# Define your VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "                    self, \n",
    "                    emb_dim, \n",
    "                    latent_dim, \n",
    "                    hidden_dim=1024,\n",
    "                    num_layers=5,\n",
    "                    rnn_type='LSTM'\n",
    "                ):                \n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        rnn_fn = {'LSTM': nn.LSTM, 'GRU': nn.GRU, 'RNN': nn.RNN}[rnn_type]\n",
    "\n",
    "        #--- ENCODER ---#\n",
    "        self.embedding = nn.Embedding(19, emb_dim)\n",
    "        self.enc_rnn =  rnn_fn(\n",
    "            input_size=emb_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers,  \n",
    "            batch_first=True,                               \n",
    "        )\n",
    "        self.enc_dense = nn.Sequential(nn.Linear(hidden_dim*num_layers, 64), nn.ReLU())\n",
    "        self.enc_dense_mu = nn.Linear(64, latent_dim)\n",
    "        self.enc_dense_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "\n",
    "        #--- DECODER ---#\n",
    "        self.dec_dense_init = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, hidden_dim * num_layers)\n",
    "        )\n",
    "        self.dec_dense_emb = nn.Linear(1, emb_dim)\n",
    "        self.dec_rnn = rnn_fn(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dec_dense_out = nn.Linear(hidden_dim, 12)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, t = self.enc_rnn(x)\n",
    "        h = t[0] if self.rnn_type == 'LSTM' else t\n",
    "        x = h.view(-1, self.hidden_dim*self.num_layers)\n",
    "        x = self.enc_dense(x)\n",
    "        mu = self.enc_dense_mu(x)\n",
    "        logvar = self.enc_dense_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        h0 = self.dec_dense_init(z).view(self.num_layers, -1, self.hidden_dim)\n",
    "        t = (h0, torch.zeros_like(h0)) if self.rnn_type == 'LSTM' else h0\n",
    "        x = self.dec_dense_emb(z.view(-1, self.latent_dim, 1))\n",
    "        x, _ = self.dec_rnn(x, t)\n",
    "        x = self.dec_dense_out(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + epsilon * std\n",
    "        return z \n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# Define your loss function\n",
    "def loss_function(reconstructed, x, mu, logvar):\n",
    "    reconstruction_loss = F.mse_loss(reconstructed, x)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  \n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n",
    "# Initialize your VAE\n",
    "device = \"cuda:1\"\n",
    "emb_dim = 10 \n",
    "latent_dim = 50\n",
    "vae = VAE(\n",
    "    emb_dim, \n",
    "    latent_dim,    \n",
    "    hidden_dim=1024,\n",
    "    num_layers=5,\n",
    "    rnn_type='RNN',\n",
    ").to(device)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# data\n",
    "data = c.Data.load(initial_path=initial_path, file_name='PET_SCORES_12_Numbers_int')\n",
    "x = data.x_train.int().to(device)\n",
    "x_test = data.x_test.int().to(device)\n",
    "y = data.y_train.float().to(device)\n",
    "y_test = data.y_test.float().to(device)\n",
    "\n",
    "x_float = x.float()\n",
    "x_test_float = x_test.float()\n",
    "\n",
    "time_start = time.time()\n",
    "loss_list = []\n",
    "for epoch in range(100):\n",
    "    ind = torch.randperm(len(x))\n",
    "    x, x_float, y = x[ind], x_float[ind], y[ind]\n",
    "    len_x = len(x)\n",
    "    for i, (x_batch, x_batch_float, y_batch) in enumerate(zip(x.split(32), x_float.split(32), y.split(32))):\n",
    "\n",
    "        #train\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mu, logvar = vae(x_batch)\n",
    "        loss = loss_function(reconstructed, x_batch_float, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #time and print\n",
    "        h, m, s = time_left(time_start, epoch, i, len_x)\n",
    "\n",
    "        #loss\n",
    "        loss = loss.item()\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        #print\n",
    "        print(f'Epoch {epoch}, batch {i}/{len_x//32}, loss={loss:.4f}, time left={h}h {m}m {s}s                                    ', end='\\r')\n",
    "\n",
    "    # print loss on test set\n",
    "    loss_last_epoch = sum(loss_list[-len_x//32:]) / (len_x//32)\n",
    "    print(f'Epoch {epoch}, loss={loss_last_epoch:.6f},                                                                                           ', end='\\n')\n",
    "\n",
    "\n",
    "# After training, you can generate new sequences\n",
    "# with torch.no_grad():\n",
    "#     z_sample = torch.randn(1, latent_dim)\n",
    "#     generated_sequence = vae.decoder(z_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "\n",
    "# Define your VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, emb_dim, latent_dim):                \n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        #--- ENCODER ---#\n",
    "        self.embedding = nn.Embedding(19, emb_dim)\n",
    "        self.enc_lstm1 = nn.LSTM(emb_dim, 64, batch_first=True, num_layers=1)\n",
    "        self.enc_lstm2 = nn.LSTM(64, 64, batch_first=True, num_layers=3)\n",
    "        self.enc_dense = nn.Sequential(nn.Linear(64, 64), nn.ReLU())\n",
    "        self.enc_dense_mu = nn.Linear(64, latent_dim)\n",
    "        self.enc_dense_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "\n",
    "        #--- DECODER ---#\n",
    "        self.dec_dense1 = nn.Sequential(nn.Linear(latent_dim, 64), nn.ReLU(), nn.Linear(64, 12*64))\n",
    "        self.dec_lstm = nn.LSTM(64, 64, batch_first=True, num_layers=5)\n",
    "        self.dec_dense2 = nn.Linear(64, 12)\n",
    "        self.dec_dense3 = nn.Linear(12, 12)\n",
    "\n",
    "\n",
    "    def decoder(self, z):\n",
    "        z = self.dec_dense1(z)\n",
    "        z = z.view(-1, 12, 64)\n",
    "        z, _ = self.dec_lstm(z)\n",
    "        z = self.dec_dense2(z[:, -1, :])\n",
    "        z = nn.ReLU()(z)\n",
    "        z = self.dec_dense3(z)\n",
    "        return z\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.enc_lstm1(x)\n",
    "        x, _ = self.enc_lstm2(x)\n",
    "        x = self.enc_dense(x[:, -1, :])\n",
    "        mu = self.enc_dense_mu(x)\n",
    "        logvar = self.enc_dense_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + epsilon * std\n",
    "        return z \n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# Define your loss function\n",
    "def loss_function(reconstructed, x, mu, logvar):\n",
    "    reconstruction_loss = F.mse_loss(reconstructed, x)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  \n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n",
    "# Initialize your VAE\n",
    "device = \"cuda:1\"\n",
    "emb_dim = 100 \n",
    "latent_dim = 12\n",
    "vae = VAE(emb_dim, latent_dim).to(device)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# data\n",
    "data = c.Data.load(initial_path=initial_path, file_name='PET_SCORES_12_Numbers_int')\n",
    "x = data.x_train.int().to(device)\n",
    "x_test = data.x_test.int().to(device)\n",
    "y = data.y_train.float().to(device)\n",
    "y_test = data.y_test.float().to(device)\n",
    "\n",
    "x_float = x.float()\n",
    "x_test_float = x_test.float()\n",
    "\n",
    "time_start = time.time()\n",
    "loss_list = []\n",
    "for epoch in range(100):\n",
    "    ind = torch.randperm(len(x))\n",
    "    x, x_float, y = x[ind], x_float[ind], y[ind]\n",
    "    len_x = len(x)\n",
    "    for i, (x_batch, x_batch_float, y_batch) in enumerate(zip(x.split(32), x_float.split(32), y.split(32))):\n",
    "\n",
    "        #train\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mu, logvar = vae(x_batch)\n",
    "        loss = loss_function(reconstructed, x_batch_float, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #time and print\n",
    "        h, m, s = time_left(time_start, epoch, i, len_x)\n",
    "\n",
    "        #loss\n",
    "        loss = loss.item()\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        #print\n",
    "        print(f'Epoch {epoch}, batch {i}/{len_x//32}, loss={loss:.4f}, time left={h}h {m}m {s}s                                    ', end='\\r')\n",
    "\n",
    "    # print loss on test set\n",
    "    loss_last_epoch = sum(loss_list[-len_x//32:]) / (len_x//32)\n",
    "    print(f'Epoch {epoch}, loss={loss_last_epoch:.6f},                                                                                           ', end='\\n')\n",
    "\n",
    "\n",
    "# After training, you can generate new sequences\n",
    "# with torch.no_grad():\n",
    "#     z_sample = torch.randn(1, latent_dim)\n",
    "#     generated_sequence = vae.decoder(z_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model(data_x_tensor[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# VAE Model Definition\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, emb_dim, latent_dim):                \n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        #--- ENCODER ---#\n",
    "        self.embedding = nn.Embedding(19, emb_dim)\n",
    "        self.enc_lstm = nn.LSTM(emb_dim, 64, batch_first=True, num_layers=3)\n",
    "        self.enc_dense = nn.Sequential(nn.Linear(64, 64), nn.ReLU())\n",
    "        self.enc_dense_mu = nn.Linear(64, latent_dim)\n",
    "        self.enc_dense_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        # #--- DECODER ---#\n",
    "        self.dec_dense1 = nn.Sequential(nn.Linear(latent_dim, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.dec_lstm = nn.LSTM(64, 64, batch_first=True, num_layers=5)\n",
    "        self.dec_dense2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 19*12), nn.ReLU(), nn.Linear(19*12, 19*12), nn.ReLU(), nn.Linear(19*12, 19*12))\n",
    "\n",
    "    def decoder(self, z):\n",
    "        z = self.dec_dense1(z)\n",
    "        z = z.unsqueeze(1).repeat(1, 64, 1)\n",
    "        z, _ = self.dec_lstm(z)\n",
    "        z = z[:, -1, :]\n",
    "        z = self.dec_dense2(z)\n",
    "        z = z.view(-1, 12, 19)\n",
    "        return F.softmax(z, dim=-1)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (h, c) = self.enc_lstm(x)\n",
    "        x = h[-1, :, :]\n",
    "        x = self.enc_dense(x)\n",
    "        mu = self.enc_dense_mu(x)\n",
    "        logvar = nn.ReLU()(self.enc_dense_logvar(x))\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + epsilon * std\n",
    "        return z \n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# Loss Function\n",
    "def vae_loss(reconstructed, original, mu, logvar):\n",
    "    flattened_original = original.view(-1)\n",
    "    flattened_reconstructed = reconstructed.view(-1, 19)\n",
    "    recon_loss = F.cross_entropy(flattened_reconstructed, flattened_original, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_loss = recon_loss + kl_divergence\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# Assume data_x is a numpy array, convert to tensor\n",
    "data = c.Data.load(initial_path=initial_path, file_name='PET_SCORES_12_Numbers_int')\n",
    "data_x_tensor = data.x_train.long().to(device)\n",
    "\n",
    "# DataLoader\n",
    "dataset = TensorDataset(data_x_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model and Optimizer Initialization\n",
    "emb_dim = 20  \n",
    "latent_dim = 12  \n",
    "model = VAE(emb_dim, latent_dim).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Loop\n",
    "time_start = time.time()\n",
    "loss_list = []\n",
    "num_epochs = 50\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, batch_data in enumerate(data_loader):\n",
    "\n",
    "        #train\n",
    "        batch_data = batch_data[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mu, logvar = model(batch_data)\n",
    "        loss = vae_loss(reconstructed, batch_data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #time and print\n",
    "        h, m, s = time_left(time_start, epoch, i, len_x)\n",
    "\n",
    "        #loss\n",
    "        loss = loss.item()\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        #print\n",
    "        print(f'Epoch {epoch+1}, batch {i}/{len(data_loader)}, loss={loss:.4f}, time left={h}h {m}m {s}s                                    ', end='\\r')\n",
    "\n",
    "    # print loss on test set\n",
    "    loss_last_epoch = sum(loss_list[-len(data_loader):]) / len(data_loader)\n",
    "    print(f'Epoch {epoch+1}, loss={loss_last_epoch:.6f},                                                                                           ', end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class one_hot_encode(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super(one_hot_encode, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        one_hot = torch.zeros(tensor.size(0), tensor.size(1), self.num_classes, device=tensor.device)\n",
    "        one_hot.scatter_(2, tensor.unsqueeze(-1).long(), 1)\n",
    "        return one_hot\n",
    "\n",
    "# VAE Model Definition\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):                \n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        #--- ENCODER ---#\n",
    "        self.enc = nn.Sequential(\n",
    "            one_hot_encode(19), nn.Flatten(), \n",
    "            nn.Linear(19*12, 64), nn.LeakyReLU(), \n",
    "            nn.Linear(64, 64), nn.LeakyReLU()\n",
    "        )\n",
    "        self.enc_dense_mu = nn.Sequential(nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, latent_dim))\n",
    "        self.enc_dense_logvar = nn.Sequential(nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, latent_dim))\n",
    "\n",
    "        # #--- DECODER ---#\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 50), nn.LeakyReLU(), \n",
    "            nn.Linear(50, 100), nn.LeakyReLU(),\n",
    "            nn.Linear(100, 200), nn.LeakyReLU(),\n",
    "            nn.Linear(200, 19*12)\n",
    "        )\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = self.enc(x)\n",
    "        mu = self.enc_dense_mu(x)\n",
    "        logvar = self.enc_dense_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decoder(self, z):\n",
    "        z = self.dec(z)\n",
    "        z = z.view(-1, 12, 19)\n",
    "        return F.softmax(z, dim=-1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + epsilon * std\n",
    "        return z \n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# Loss Function\n",
    "def vae_loss(reconstructed, original, mu, logvar):\n",
    "    flattened_original = original.view(-1)\n",
    "    flattened_reconstructed = reconstructed.view(-1, 19)\n",
    "    recon_loss = F.cross_entropy(flattened_reconstructed, flattened_original) #, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_loss = recon_loss + kl_divergence\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# device = \"cuda:1\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# Assume data_x is a numpy array, convert to tensor\n",
    "data = c.Data.load(initial_path=initial_path, file_name='PET_SCORES_12_Numbers_int')\n",
    "data_x_tensor = data.x_train.long().to(device)\n",
    "\n",
    "# DataLoader\n",
    "dataset = TensorDataset(data_x_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model and Optimizer Initialization \n",
    "latent_dim = 6  \n",
    "model = VAE(latent_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Loop\n",
    "time_start = time.time()\n",
    "loss_list = []\n",
    "num_epochs = 50\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, batch_data in enumerate(data_loader):\n",
    "\n",
    "        #train\n",
    "        batch_data = batch_data[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mu, logvar = model(batch_data)\n",
    "        loss = vae_loss(reconstructed, batch_data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #time and print\n",
    "        h, m, s = time_left(time_start, epoch, i, len_x)\n",
    "\n",
    "        #loss\n",
    "        loss = loss.item()\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        #print\n",
    "        print(f'Epoch {epoch+1}, batch {i}/{len(data_loader)}, loss={loss:.4f}, time left={h}h {m}m {s}s                                    ', end='\\r')\n",
    "\n",
    "    # print loss on test set\n",
    "    loss_last_epoch = sum(loss_list[-len(data_loader):]) / len(data_loader)\n",
    "    print(f'Epoch {epoch+1}, loss={loss_last_epoch:.6f},                                                                                           ', end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.argmax(model.to('cuda')(data.x_test[160:165].to('cuda'))[0], dim=-1)\n",
    "#round to closest integer\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x_test[162:165].to('cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JL_Pennylane",
   "language": "python",
   "name": "jl_pennylane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
