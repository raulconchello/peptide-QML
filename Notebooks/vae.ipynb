{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_path = 'peptide-QML'\n",
    "# initial_path = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, pickle\n",
    "sys.path.append(initial_path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from my_code import helper_classes as c\n",
    "from my_code import quantum_nodes as q\n",
    "# from my_code import pytorch_model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_left(time_start, n_epochs_total, n_batches_total, n_epochs_done, current_batch):\n",
    "    time_left = (time.time() - time_start) * (n_epochs_total*n_batches_total / (n_epochs_done*n_batches_total + current_batch) - 1)\n",
    "    total_hours = int(time_left // 3600)\n",
    "    total_minutes = int((time_left - total_hours * 3600) // 60)\n",
    "    total_seconds = int(time_left - total_hours * 3600 - total_minutes * 60)\n",
    "\n",
    "    # remaining time for the current epoch\n",
    "    time_left_epoch = (time.time() - time_start) / (n_epochs_done*n_batches_total + current_batch) * (n_batches_total - current_batch)\n",
    "    epoch_hours = int(time_left_epoch // 3600)\n",
    "    epoch_minutes = int((time_left_epoch - epoch_hours * 3600) // 60)\n",
    "    epoch_seconds = int(time_left_epoch - epoch_hours * 3600 - epoch_minutes * 60)\n",
    "\n",
    "    return epoch_hours, epoch_minutes, epoch_seconds, total_hours, total_minutes, total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Score Predictor model\n",
    "class ScorePredictor(nn.Module):\n",
    "    def __init__(self, latent_dim:int):\n",
    "        super(ScorePredictor, self).__init__()\n",
    "        \n",
    "        self.quantum_layer = q.circuit(\n",
    "            n_qubits = int(q.np.ceil(q.np.log2(latent_dim))),\n",
    "            device = \"default.qubit.torch\",\n",
    "            device_options = {'shots': None},\n",
    "            embedding = q.parts.AmplitudeEmbedding,\n",
    "            # embedding_ansatz = sweep_point['ansatz'],\n",
    "            block_ansatz = q.parts.Ansatz_11,\n",
    "            final_ansatz = q.parts.Ansatz_11, \n",
    "            measurement = q.parts.Measurement('Z', 1),\n",
    "            # embedding_n_layers = sweep_point['embedding_n_layers'],\n",
    "            # different_inputs_per_layer = False,\n",
    "            block_n_layers = 10,\n",
    "            # wrapper_qlayer = pw.QLayerEmpty,\n",
    "        )()\n",
    "        # self.quantum_layer = nn.Linear(latent_dim, 1)\n",
    "        self.post_quantum = nn.Linear(1, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(predicted_socre, real_socre, reduction:str='mean'):\n",
    "        score_loss = F.mse_loss(predicted_socre, real_socre.float(), reduction=reduction)\n",
    "        return score_loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quantum_layer(x)\n",
    "        x = self.post_quantum(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "\n",
    "# helper function\n",
    "def dense_sequence(in_dim, out_dim, n_dense_layers, activation):\n",
    "    \"\"\"\n",
    "    dense_sequence(100, 53, 3, nn.ReLU) returns:\n",
    "    [   Linear(in_features=100, out_features=153, bias=True), ReLU(),\n",
    "        Linear(in_features=153, out_features=153, bias=True), ReLU(),\n",
    "        Linear(in_features=153, out_features=53,  bias=True)             ]\n",
    "    \"\"\"\n",
    "\n",
    "    steps = q.np.linspace(in_dim, out_dim, n_dense_layers+1, dtype=int) if in_dim != out_dim else [in_dim] * (n_dense_layers+1)\n",
    "    steps = [(i,j) for i,j in zip(steps[:-1], steps[1:])]\n",
    "    sequence = []\n",
    "    for in_units, out_units in steps[:-1]:\n",
    "        sequence += [nn.Linear(in_units, out_units), activation()]\n",
    "    sequence += [nn.Linear(*steps[-1])]\n",
    "\n",
    "    return nn.Sequential(*sequence)\n",
    "\n",
    "def dense_sequence_XL(in_dim, out_dim, n_dense_layers, activation):\n",
    "    \"\"\"\n",
    "    dense_sequence_XL(100, 53, 3, nn.ReLU) returns:\n",
    "    [   Linear(in_features=100, out_features=84, bias=True), ReLU(),\n",
    "        Linear(in_features=84,  out_features=68, bias=True), ReLU(),\n",
    "        Linear(in_features=68,  out_features=53, bias=True)             ]\n",
    "    \"\"\"\n",
    "\n",
    "    sequence = []\n",
    "    for i, (s_dim, f_dim) in enumerate([(in_dim, in_dim + out_dim), (in_dim + out_dim, out_dim)]):\n",
    "        \n",
    "        steps = q.np.linspace(s_dim, f_dim, n_dense_layers//2+1, dtype=int) if in_dim != out_dim else [in_dim] * (n_dense_layers+1)\n",
    "        steps = [(i,j) for i,j in zip(steps[:-1], steps[1:])]     \n",
    "\n",
    "        for in_units, out_units in (steps[:-1] if i == 1 else steps):\n",
    "            sequence += [nn.Linear(in_units, out_units), activation()]\n",
    "\n",
    "        if i == 0:  sequence += [nn.Linear(out_units, out_units), activation()] if n_dense_layers % 2 != 0 else []\n",
    "        else:       sequence += [nn.Linear(*steps[-1])]\n",
    "\n",
    "    return nn.Sequential(*sequence)\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    N_EMB = 18\n",
    "    RNN_types_dict = {'LSTM': nn.LSTM, 'GRU': nn.GRU, 'RNN': nn.RNN}\n",
    "\n",
    "    def __init__(self, emb_dim:int, latent_dim:int, output_dim:int, n_dense_layers:int, RNN_type:str, RNN_units:list, bidirectional:bool=True, dropout:float=0, num_layers:int=2, one_hot:bool=False):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Define the hyperparameters\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_dense_layers = n_dense_layers\n",
    "        self.RNN_type = RNN_type\n",
    "        self.RNN_units = RNN_units if isinstance(RNN_units, list) else [RNN_units]\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.one_hot = one_hot\n",
    "        # TODO: add temperature for softmax\n",
    "\n",
    "        # Encoder and Decoder\n",
    "        self.encoder = VAE.Encoder(emb_dim, latent_dim, n_dense_layers, RNN_type, RNN_units, bidirectional, dropout, num_layers)\n",
    "        self.decoder = VAE.Decoder(latent_dim, output_dim, n_dense_layers, RNN_type, RNN_units[::-1], bidirectional, dropout, num_layers, one_hot)\n",
    "\n",
    "    # Define the Encoder\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, emb_dim:int, latent_dim:int, n_dense_layers:int, RNN_type:str, RNN_units:list, bidirectional:bool, dropout:float, num_layers:int):\n",
    "            super(VAE.Encoder, self).__init__()\n",
    "\n",
    "            # Define the hyperparameters\n",
    "            rnn_layer = VAE.RNN_types_dict[RNN_type]\n",
    "            lstm_out_dim = RNN_units[-1] * 2 if bidirectional else RNN_units[-1]\n",
    "            self.RNN_units = RNN_units\n",
    "\n",
    "            # layers\n",
    "            self.embedding = nn.Embedding(VAE.N_EMB, emb_dim)\n",
    "\n",
    "            in_units = emb_dim\n",
    "            for i, out_units in enumerate(RNN_units):\n",
    "                setattr(self, f'lstm_{i}', rnn_layer(in_units, out_units, batch_first=True, bidirectional=bidirectional, dropout=dropout, num_layers=num_layers)) \n",
    "                in_units = out_units * 2 if bidirectional else out_units\n",
    "\n",
    "            self.fc_post = nn.Sequential(nn.Linear(lstm_out_dim, lstm_out_dim), nn.ReLU())\n",
    "            self.fc_mean = dense_sequence(lstm_out_dim, latent_dim, n_dense_layers, nn.ReLU)\n",
    "            self.fc_log_var = dense_sequence(lstm_out_dim, latent_dim, n_dense_layers, nn.ReLU)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x)\n",
    "            for i in range(len(self.RNN_units)):\n",
    "                x, _ = getattr(self, f'lstm_{i}')(x)\n",
    "            x = x[:, -1, :]  # Take the last time step output\n",
    "            x = self.fc_post(x)\n",
    "            z_mean = self.fc_mean(x)\n",
    "            z_log_var = self.fc_log_var(x)\n",
    "            return z_mean, z_log_var\n",
    "        \n",
    "    # Define the Encoder\n",
    "    class Encoder_conv(nn.Module):\n",
    "        pass\n",
    "        # TODO\n",
    "\n",
    "    # Define the Decoder\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, latent_dim:int, output_dim:int, n_dense_layers:int, RNN_type:str, RNN_units:list, bidirectional:bool, dropout:float, num_layers:int, one_hot:bool):\n",
    "            super(VAE.Decoder, self).__init__()\n",
    "\n",
    "            # Define the hyperparameters\n",
    "            self.output_dim = output_dim\n",
    "            self.one_hot = one_hot\n",
    "            rnn_layer = VAE.RNN_types_dict[RNN_type]\n",
    "            lstm_out_dim = RNN_units[-1] * 2 if bidirectional else RNN_units[-1]\n",
    "            self.RNN_units = RNN_units\n",
    "\n",
    "            # layers\n",
    "            self.fc_pre = dense_sequence(latent_dim, latent_dim, n_dense_layers, nn.ReLU)\n",
    "\n",
    "            in_units = latent_dim\n",
    "            for i, out_units in enumerate(RNN_units):\n",
    "                setattr(self, f'lstm_{i}', rnn_layer(in_units, out_units, batch_first=True, bidirectional=bidirectional, dropout=dropout, num_layers=num_layers)) \n",
    "                in_units = out_units * 2 if bidirectional else out_units\n",
    "\n",
    "            # output_dim = output_dim if not one_hot else VAE.N_EMB*output_dim\n",
    "            output_dim = VAE.N_EMB if one_hot else output_dim\n",
    "            self.fc_post = dense_sequence(lstm_out_dim, output_dim, n_dense_layers, nn.ReLU)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc_pre(x)\n",
    "            x = x.unsqueeze(1).repeat(1, self.output_dim, 1)\n",
    "            \n",
    "            for i in range(len(self.RNN_units)):\n",
    "                x, _ = getattr(self, f'lstm_{i}')(x)\n",
    "                \n",
    "            if self.one_hot:                \n",
    "                out_reshape = x.contiguous().view(-1, x.size(-1))\n",
    "                y0 = F.softmax(self.fc_post(out_reshape), dim=1)\n",
    "                x = y0.contiguous().view(x.size(0), -1, y0.size(-1))\n",
    "            else:\n",
    "                x = self.fc_post(x[:, -1, :])\n",
    "                x = F.relu(x)\n",
    "            return x\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(z_mean, z_log_var):\n",
    "        epsilon = 1e-2*torch.randn_like(z_mean)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_function(reconstructed, x, mu=None, logvar=None, one_hot:bool=False, reduction:str='sum', kl_weight:float=1.0):\n",
    "        if not one_hot:\n",
    "            reconstruction_loss = F.mse_loss(reconstructed, x.float(), reduction=reduction)\n",
    "        else:\n",
    "            reconstruction_loss = F.cross_entropy(reconstructed.view(-1, VAE.N_EMB), x.view(-1).long(), reduction=reduction)\n",
    "            \n",
    "        if mu is None or logvar is None or kl_weight == 0:\n",
    "            return reconstruction_loss\n",
    "        \n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  \n",
    "        return reconstruction_loss + kl_divergence*kl_weight\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_decoded_outputs(reconstructed, one_hot:bool=False):\n",
    "        if not one_hot:\n",
    "            return torch.round(reconstructed).int()\n",
    "        else:\n",
    "            return torch.argmax(reconstructed, dim=-1).int()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded_sequence = self.decoder(z)\n",
    "        return decoded_sequence, z_mean, z_log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_points = [\n",
    "    {\n",
    "        'emb_dim': 200,\n",
    "        'latent_dim': 16,\n",
    "        'n_dense_layers': 1,\n",
    "        'RNN_type': 'LSTM',\n",
    "        'RNN_units': [500],\n",
    "        'bidirectional': True,\n",
    "        'dropout': 0,\n",
    "        'num_layers': 2,\n",
    "        'one_hot': False,\n",
    "        'n_epochs': 500,\n",
    "        'lr': 3e-4,\n",
    "        'save_name': 'vae_2_1.pickle',\n",
    "        'score_predictor': True,\n",
    "    },\n",
    "    {\n",
    "        'emb_dim': 200,\n",
    "        'latent_dim': 64,\n",
    "        'n_dense_layers': 1,\n",
    "        'RNN_type': 'LSTM',\n",
    "        'RNN_units': [1000],\n",
    "        'bidirectional': True,\n",
    "        'dropout': 0,\n",
    "        'num_layers': 2,\n",
    "        'one_hot': False,\n",
    "        'n_epochs': 500,\n",
    "        'lr': 3e-4,\n",
    "        'save_name': 'vae_2_2.pickle',\n",
    "        'score_predictor': True,\n",
    "    },\n",
    "    {\n",
    "        'emb_dim': 200,\n",
    "        'latent_dim': 64,\n",
    "        'n_dense_layers': 2,\n",
    "        'RNN_type': 'LSTM',\n",
    "        'RNN_units': [1000],\n",
    "        'bidirectional': True,\n",
    "        'dropout': 0,\n",
    "        'num_layers': 2,\n",
    "        'one_hot': False,\n",
    "        'n_epochs': 500,\n",
    "        'lr': 3e-4,\n",
    "        'save_name': 'vae_2_3.pickle',\n",
    "        'score_predictor': True,\n",
    "    },\n",
    "    {\n",
    "        'emb_dim': 200,\n",
    "        'latent_dim': 16,\n",
    "        'n_dense_layers': 1,\n",
    "        'RNN_type': 'GRU',\n",
    "        'RNN_units': [500],\n",
    "        'bidirectional': True,\n",
    "        'dropout': 0,\n",
    "        'num_layers': 3,\n",
    "        'one_hot': False,\n",
    "        'n_epochs': 500,\n",
    "        'lr': 3e-4,\n",
    "        'save_name': 'vae_2_4.pickle',\n",
    "        'score_predictor': True,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ## ----- NEW SWEEP POINT ----- ## \n",
      "\n",
      "{'emb_dim': 200, 'latent_dim': 16, 'n_dense_layers': 1, 'RNN_type': 'LSTM', 'RNN_units': [500], 'bidirectional': True, 'dropout': 0, 'num_layers': 2, 'one_hot': False, 'n_epochs': 500, 'lr': 0.0003, 'save_name': 'vae_2_1.pickle', 'score_predictor': True} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1/500, \t train score predictor = True\n",
      "Epoch 1/500, \t loss=1330.148796, \t loss test=358.598327, \t accuracy test=0.102362, \t loss score=784.720886,                                                       \n",
      "\n",
      "\n",
      "Epoch 2/500, \t train score predictor = True\n",
      "Epoch 2/500, \t loss=1024.833877, \t loss test=322.155840, \t accuracy test=0.111702, \t loss score=643.532593,                                                       \n",
      "\n",
      "\n",
      "Epoch 3/500, \t train score predictor = True\n",
      "Epoch 3/500, \t loss=873.446588, \t loss test=297.394291, \t accuracy test=0.118854, \t loss score=522.468201,                                                       \n",
      "\n",
      "\n",
      "Epoch 4/500, \t train score predictor = False\n",
      "Epoch 4/500, \t loss=290.746878, \t loss test=291.310269, \t accuracy test=0.119554, \t loss score=523.027405,                                                       \n",
      "\n",
      "\n",
      "Epoch 5/500, \t train score predictor = True\n",
      "Epoch 5/500, \t loss=749.059685, \t loss test=292.430184, \t accuracy test=0.118613, \t loss score=421.453064,                                                       \n",
      "\n",
      "\n",
      "Epoch 6/500, \t train score predictor = False\n",
      "Epoch 6/500, \t loss=283.686987, \t loss test=282.765978, \t accuracy test=0.127515, \t loss score=422.107361,                                                       \n",
      "\n",
      "\n",
      "Epoch 7/500, \t train score predictor = False\n",
      "Epoch 7/500, \t loss=281.135504, \t loss test=281.162303, \t accuracy test=0.131059, \t loss score=423.443665,                                                       \n",
      "\n",
      "\n",
      "Epoch 8/500, \t train score predictor = False\n",
      "Epoch 8/500, \t loss=279.287599, \t loss test=278.862894, \t accuracy test=0.134668, \t loss score=425.299225,                                                       \n",
      "\n",
      "\n",
      "Epoch 9/500, \t train score predictor = True\n",
      "Epoch 9/500, \t loss=649.363214, \t loss test=276.597835, \t accuracy test=0.137620, \t loss score=340.181885,                                                       \n",
      "\n",
      "\n",
      "Epoch 10/500, \t train score predictor = True\n",
      "Epoch 10/500, \t loss=577.846449, \t loss test=279.406759, \t accuracy test=0.138539, \t loss score=278.602051,                                                       \n",
      "\n",
      "\n",
      "Epoch 11/500, \t train score predictor = False\n",
      "Epoch 11/500, \t loss=275.344419, \t loss test=276.774049, \t accuracy test=0.149234, \t loss score=279.292114,                                                       \n",
      "\n",
      "\n",
      "Epoch 12/500, \t train score predictor = True\n",
      "Epoch 12/500, \t loss=525.076927, \t loss test=274.733678, \t accuracy test=0.145341, \t loss score=236.087936,                                                       \n",
      "\n",
      "\n",
      "Epoch 13/500, \t train score predictor = True\n",
      "Epoch 13/500, \t loss=489.800155, \t loss test=280.438255, \t accuracy test=0.142913, \t loss score=182.464096,                                                       \n",
      "\n",
      "\n",
      "Epoch 14/500, \t train score predictor = True\n",
      "Epoch 14/500, \t loss=422.597042, \t loss test=275.239009, \t accuracy test=0.148010, \t loss score=128.994080,                                                       \n",
      "\n",
      "\n",
      "Epoch 15/500, \t train score predictor = False\n",
      "Epoch 15/500, \t loss=269.885858, \t loss test=267.590436, \t accuracy test=0.188123, \t loss score=198.795822,                                                       \n",
      "\n",
      "\n",
      "Epoch 16/500, \t train score predictor = False\n",
      "Epoch 16/500, \t loss=265.245483, \t loss test=265.076903, \t accuracy test=0.209405, \t loss score=200.230316,                                                       \n",
      "\n",
      "\n",
      "Epoch 17/500, \t train score predictor = False\n",
      "Epoch 17/500, \t loss=263.944981, \t loss test=264.893635, \t accuracy test=0.210302, \t loss score=203.453995,                                                       \n",
      "\n",
      "\n",
      "Epoch 18/500, \t train score predictor = True\n",
      "Epoch 18/500, \t loss=385.633976, \t loss test=271.068438, \t accuracy test=0.179199, \t loss score=97.732430,                                                       \n",
      "\n",
      "\n",
      "Epoch 19/500, \t train score predictor = True\n",
      "Epoch 19/500, \t loss=351.132014, \t loss test=272.376624, \t accuracy test=0.176006, \t loss score=73.103432,                                                       \n",
      "\n",
      "\n",
      "Epoch 20/500, \t train score predictor = True\n",
      "Epoch 20/500, \t loss=327.553908, \t loss test=272.305955, \t accuracy test=0.176684, \t loss score=53.356037,                                                       \n",
      "\n",
      "\n",
      "Epoch 21/500, \t train score predictor = False\n",
      "Epoch 21/500, \t loss=262.993239, \t loss test=264.429183, \t accuracy test=0.218570, \t loss score=257.610168,                                                       \n",
      "\n",
      "\n",
      "Epoch 22/500, \t train score predictor = False\n",
      "Epoch 22/500, \t loss=260.769075, \t loss test=261.772785, \t accuracy test=0.232896, \t loss score=248.744553,                                                       \n",
      "\n",
      "\n",
      "Epoch 23/500, \t train score predictor = True\n",
      "Epoch 23/500, \t loss=310.568222, \t loss test=263.730955, \t accuracy test=0.219904, \t loss score=39.790379,                                                       \n",
      "\n",
      "\n",
      "Epoch 24/500, \t train score predictor = False\n",
      "Epoch 24/500, \t loss=259.517173, \t loss test=261.420948, \t accuracy test=0.232983, \t loss score=283.024689,                                                       \n",
      "\n",
      "\n",
      "Epoch 25/500, \t train score predictor = False\n",
      "Epoch 25/500, \t loss=255.080153, \t loss test=253.509957, \t accuracy test=0.263233, \t loss score=287.265381,                                                       \n",
      "\n",
      "\n",
      "Epoch 26/500, \t train score predictor = False\n",
      "Epoch 26/500, \t loss=251.151856, \t loss test=252.724918, \t accuracy test=0.267017, \t loss score=278.928741,                                                       \n",
      "\n",
      "\n",
      "Epoch 27/500, \t train score predictor = False\n",
      "Epoch 27/500, \t loss=248.649786, \t loss test=250.234022, \t accuracy test=0.280490, \t loss score=275.652313,                                                       \n",
      "\n",
      "\n",
      "Epoch 28/500, \t train score predictor = False\n",
      "Epoch 28/500, \t loss=247.012230, \t loss test=250.533678, \t accuracy test=0.286045, \t loss score=271.936005,                                                       \n",
      "\n",
      "\n",
      "Epoch 29/500, \t train score predictor = False\n",
      "Epoch 29/500, \t loss=245.517441, \t loss test=247.128707, \t accuracy test=0.292738, \t loss score=271.476013,                                                       \n",
      "\n",
      "\n",
      "Epoch 30/500, \t train score predictor = False\n",
      "Epoch 30/500, \t loss=244.547118, \t loss test=246.056332, \t accuracy test=0.306868, \t loss score=263.175842,                                                       \n",
      "\n",
      "\n",
      "Epoch 31/500, \t train score predictor = False\n",
      "Epoch 31/500, \t loss=243.894915, \t loss test=245.153068, \t accuracy test=0.306540, \t loss score=259.030334,                                                       \n",
      "\n",
      "\n",
      "Epoch 32/500, \t train score predictor = False\n",
      "Epoch 32/500, \t loss=243.151270, \t loss test=245.555741, \t accuracy test=0.309121, \t loss score=261.282043,                                                       \n",
      "\n",
      "\n",
      "Epoch 33/500, \t train score predictor = False\n",
      "Epoch 33/500, \t loss=242.716427, \t loss test=244.957628, \t accuracy test=0.320407, \t loss score=255.175400,                                                       \n",
      "\n",
      "\n",
      "Epoch 34/500, \t train score predictor = False\n",
      "Epoch 34/500, \t loss=242.242945, \t loss test=243.904134, \t accuracy test=0.311111, \t loss score=254.236053,                                                       \n",
      "\n",
      "\n",
      "Epoch 35/500, \t train score predictor = False\n",
      "Epoch 35/500, \t loss=241.723991, \t loss test=244.794291, \t accuracy test=0.318854, \t loss score=247.820343,                                                       \n",
      "\n",
      "\n",
      "Epoch 36/500, \t train score predictor = False\n",
      "Epoch 36/500, \t loss=241.281597, \t loss test=243.650886, \t accuracy test=0.326290, \t loss score=243.043243,                                                       \n",
      "\n",
      "\n",
      "Epoch 37/500, \t train score predictor = False\n",
      "Epoch 37/500, \t loss=240.839604, \t loss test=242.156841, \t accuracy test=0.329834, \t loss score=237.194687,                                                       \n",
      "\n",
      "\n",
      "Epoch 38/500, \t train score predictor = False\n",
      "Epoch 38/500, \t loss=240.482977, \t loss test=242.229265, \t accuracy test=0.329899, \t loss score=231.022003,                                                       \n",
      "\n",
      "\n",
      "Epoch 39/500, \t train score predictor = False\n",
      "Epoch 39/500, \t loss=240.170828, \t loss test=241.353035, \t accuracy test=0.341120, \t loss score=226.466858,                                                       \n",
      "\n",
      "\n",
      "Epoch 40/500, \t train score predictor = False\n",
      "Epoch 40/500, \t loss=239.882769, \t loss test=246.490420, \t accuracy test=0.314589, \t loss score=227.154648,                                                       \n",
      "\n",
      "\n",
      "Epoch 41/500, \t train score predictor = False\n",
      "Epoch 41/500, \t loss=239.783285, \t loss test=242.714354, \t accuracy test=0.325241, \t loss score=227.366409,                                                       \n",
      "\n",
      "\n",
      "Epoch 42/500, \t train score predictor = False\n",
      "Epoch 42/500, \t loss=239.478245, \t loss test=240.848737, \t accuracy test=0.345013, \t loss score=226.259338,                                                       \n",
      "\n",
      "\n",
      "Epoch 43/500, \t train score predictor = False\n",
      "Epoch 43/500, \t loss=239.427026, \t loss test=240.859203, \t accuracy test=0.340551, \t loss score=227.950760,                                                       \n",
      "\n",
      "\n",
      "Epoch 44/500, \t train score predictor = False\n",
      "Epoch 44/500, \t loss=239.161855, \t loss test=242.898360, \t accuracy test=0.336811, \t loss score=224.927170,                                                       \n",
      "\n",
      "\n",
      "Epoch 45/500, \t train score predictor = True\n",
      "Epoch 45/500, \t loss=325.705953, \t loss test=253.492503, \t accuracy test=0.240704, \t loss score=49.338566,                                                       \n",
      "\n",
      "\n",
      "Epoch 46/500, \t train score predictor = False\n",
      "Epoch 46/500, \t loss=244.073363, \t loss test=242.866814, \t accuracy test=0.308027, \t loss score=252.241730,                                                       \n",
      "\n",
      "\n",
      "Epoch 47/500, \t train score predictor = True\n",
      "Epoch 47/500, \t loss=293.093856, \t loss test=249.290289, \t accuracy test=0.270932, \t loss score=39.222099,                                                       \n",
      "\n",
      "\n",
      "Epoch 48/500, \t train score predictor = False\n",
      "Epoch 48/500, \t loss=240.642884, \t loss test=242.585712, \t accuracy test=0.318220, \t loss score=291.019135,                                                       \n",
      "\n",
      "\n",
      "Epoch 49/500, \t train score predictor = True\n",
      "Epoch 49/500, \t loss=278.682936, \t loss test=249.165502, \t accuracy test=0.297222, \t loss score=32.290039,                                                       \n",
      "\n",
      "\n",
      "Epoch 50/500, \t train score predictor = False\n",
      "Epoch 50/500, \t loss=239.538031, \t loss test=242.240059, \t accuracy test=0.330512, \t loss score=330.011688,                                                       \n",
      "\n",
      "\n",
      "Epoch 51/500, \t train score predictor = False\n",
      "Epoch 51/500, \t loss=238.446257, \t loss test=239.917487, \t accuracy test=0.350612, \t loss score=313.487366,                                                       \n",
      "\n",
      "\n",
      "Epoch 52/500, \t train score predictor = False\n",
      "Epoch 52/500, \t loss=236.870638, \t loss test=238.832218, \t accuracy test=0.364392, \t loss score=307.734436,                                                       \n",
      "\n",
      "\n",
      "Epoch 53/500, \t train score predictor = True\n",
      "Epoch 53/500, \t loss=274.007425, \t loss test=241.461565, \t accuracy test=0.339326, \t loss score=28.836063,                                                       \n",
      "\n",
      "\n",
      "Epoch 54/500, \t train score predictor = False\n",
      "Epoch 54/500, \t loss=237.575227, \t loss test=238.147195, \t accuracy test=0.379943, \t loss score=354.930786,                                                       \n",
      "\n",
      "\n",
      "Epoch 55/500, \t train score predictor = False\n",
      "Epoch 55/500, \t loss=235.994931, \t loss test=239.540486, \t accuracy test=0.371719, \t loss score=344.145599,                                                       \n",
      "\n",
      "\n",
      "Epoch 56/500, \t train score predictor = False\n",
      "Epoch 56/500, \t loss=235.588212, \t loss test=237.623983, \t accuracy test=0.393832, \t loss score=333.361816,                                                       \n",
      "\n",
      "\n",
      "Epoch 57/500, \t train score predictor = False\n",
      "Epoch 57/500, \t loss=235.200792, \t loss test=237.361663, \t accuracy test=0.395276, \t loss score=324.944489,                                                       \n",
      "\n",
      "\n",
      "Epoch 58/500, \t train score predictor = False\n",
      "Epoch 58/500, \t loss=235.422009, \t loss test=237.846900, \t accuracy test=0.394794, \t loss score=316.867889,                                                       \n",
      "\n",
      "\n",
      "Epoch 59/500, \t train score predictor = False\n",
      "Epoch 59/500, \t loss=234.855534, \t loss test=239.109367, \t accuracy test=0.370035, \t loss score=308.086426,                                                       \n",
      "\n",
      "\n",
      "Epoch 60/500, \t train score predictor = False\n",
      "Epoch 60/500, \t loss=234.549968, \t loss test=237.314879, \t accuracy test=0.394904, \t loss score=302.261688,                                                       \n",
      "\n",
      "\n",
      "Epoch 61/500, \t train score predictor = True\n",
      "Epoch 61/500, \t loss=276.734084, \t loss test=240.733005, \t accuracy test=0.347003, \t loss score=28.920862,                                                       \n",
      "\n",
      "\n",
      "Epoch 62/500, \t train score predictor = False\n",
      "Epoch 62/500, \t loss=235.631467, \t loss test=236.162073, \t accuracy test=0.406605, \t loss score=343.793152,                                                       \n",
      "\n",
      "\n",
      "Epoch 63/500, \t train score predictor = False\n",
      "Epoch 63/500, \t loss=234.300002, \t loss test=235.484367, \t accuracy test=0.422157, \t loss score=334.670013,                                                       \n",
      "\n",
      "\n",
      "Epoch 64/500, \t train score predictor = False\n",
      "Epoch 64/500, \t loss=234.048949, \t loss test=235.653461, \t accuracy test=0.423075, \t loss score=326.414307,                                                       \n",
      "\n",
      "\n",
      "Epoch 65/500, \t train score predictor = False\n",
      "Epoch 65/500, batch 5359/5359, loss=189.3165, \t total time left = 41h 8m 3s, \t epoch time left = 0h 0m 0s                            \r"
     ]
    }
   ],
   "source": [
    "for sweep_point in sweep_points:\n",
    "\n",
    "    print('\\n\\n\\n ## ----- NEW SWEEP POINT ----- ## \\n') \n",
    "    print(sweep_point, '\\n')   \n",
    "\n",
    "    # Create the VAE model and get data\n",
    "    device = \"cuda:1\"\n",
    "    # device = \"cpu\"\n",
    "\n",
    "    data = c.Data.load(initial_path=initial_path, file_name='PET_SCORES_12')\n",
    "    data.set_test_ptc(0.1)\n",
    "    data.to(device)\n",
    "    data_loader = data.get_loader(batch_size=64, shuffle=True)\n",
    "\n",
    "    # Define models\n",
    "    vae_model = VAE(\n",
    "        emb_dim = sweep_point['emb_dim'],\n",
    "        latent_dim = sweep_point['latent_dim'],\n",
    "        output_dim = len(data.x_train[0]), #12\n",
    "        n_dense_layers = sweep_point['n_dense_layers'],\n",
    "        RNN_type = sweep_point['RNN_type'],\n",
    "        RNN_units = sweep_point['RNN_units'],\n",
    "        bidirectional = sweep_point['bidirectional'],\n",
    "        dropout = sweep_point['dropout'],\n",
    "        num_layers = sweep_point['num_layers'],\n",
    "        one_hot = sweep_point['one_hot'],\n",
    "    ).to(device)\n",
    "    score_predictor = ScorePredictor(vae_model.latent_dim).to(device)\n",
    "\n",
    "    # Define your optimizer\n",
    "    optimizer_vae = optim.Adam(vae_model.parameters(), lr=sweep_point['lr'])\n",
    "    optimizer_score = optim.Adam(score_predictor.parameters(), lr=sweep_point['lr'])\n",
    "\n",
    "    #training\n",
    "    vae_model.losses = {\n",
    "        'batch': [],\n",
    "        'epoch': [],\n",
    "        'test': [],\n",
    "        'score': [],\n",
    "        'accuracy': [],\n",
    "    }\n",
    "    time_start, n_epochs, n_batches = time.time(), sweep_point['n_epochs'], len(data_loader)\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # train score predictor?\n",
    "        if sweep_point['score_predictor']:\n",
    "            train_score_predictor = torch.rand(1).item() < 1/4 if epoch > 2 else True\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, \\t train score predictor = {train_score_predictor}')\n",
    "        else:\n",
    "            train_score_predictor = False\n",
    "\n",
    "        vae_model.train()\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "\n",
    "            #train\n",
    "            optimizer_vae.zero_grad()\n",
    "            x_reconstructed, mu, logvar = vae_model(x)\n",
    "            loss_vae = VAE.loss_function(x_reconstructed, x, mu, logvar, one_hot=vae_model.one_hot, reduction='sum')\n",
    "\n",
    "            if train_score_predictor:\n",
    "                optimizer_score.zero_grad()\n",
    "                pred_score = score_predictor(VAE.reparameterize(mu, logvar)) \n",
    "                loss_score = ScorePredictor.loss_function(pred_score, y, reduction='sum')\n",
    "                loss = loss_vae + loss_score\n",
    "            else:\n",
    "                loss = loss_vae\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_vae.step()\n",
    "            if train_score_predictor: optimizer_score.step()\n",
    "\n",
    "            #time and print\n",
    "            h, m, s, th, tm, ts = time_left(time_start, n_epochs, n_batches, epoch, i+1)\n",
    "\n",
    "            #loss\n",
    "            loss = loss.item()\n",
    "            vae_model.losses['batch'].append(loss)\n",
    "            \n",
    "            #print\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, batch {i+1}/{n_batches}, loss={loss/data_loader.batch_size:.4f}, \\t total time left = {th}h {tm}m {ts}s, \\t epoch time left = {h}h {m}m {s}s                         ', end='\\r')\n",
    "\n",
    "        #validation\n",
    "        vae_model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_reconstructed, mu, logvar = vae_model(data.x_test_ptc)\n",
    "            loss_test = VAE.loss_function(x_reconstructed, data.x_test_ptc, mu, logvar, one_hot=vae_model.one_hot, reduction='sum')\n",
    "            vae_model.losses['test'].append(loss_test.item() / len(data.x_test_ptc))\n",
    "\n",
    "            prediction = VAE.process_decoded_outputs(x_reconstructed, one_hot=vae_model.one_hot)\n",
    "            accuracy = (prediction == data.x_test_ptc).float().mean().item()\n",
    "            vae_model.losses['accuracy'].append(accuracy)\n",
    "\n",
    "            if train_score_predictor or (epoch>0 and vae_model.losses['score'][-1]>0):\n",
    "                loss_score = ScorePredictor.loss_function(score_predictor(mu), data.y_test_ptc, reduction='mean')\n",
    "                vae_model.losses['score'].append(loss_score.item())\n",
    "            else:\n",
    "                vae_model.losses['score'].append(0)\n",
    "\n",
    "        # print loss on test set\n",
    "        vae_model.losses['epoch'].append(sum(vae_model.losses['batch'][-n_batches:]) / (n_batches*data_loader.batch_size))\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \\t loss={vae_model.losses['epoch'][-1]:.6f}, \\t loss test={vae_model.losses['test'][-1]:.6f}, \\t accuracy test={vae_model.losses['accuracy'][-1]:.6f}, \\t loss score={vae_model.losses['score'][-1]:.6f},                                                       \", end='\\n')\n",
    "        \n",
    "        #save\n",
    "        pickle.dump(vae_model, open(initial_path+'/saved/Pickle/VAE-'+sweep_point['save_name'], 'wb'))\n",
    "        pickle.dump(score_predictor.state_dict(), open(initial_path+'/saved/Pickle/SP-'+sweep_point['save_name'], 'wb'))\n",
    "\n",
    "        # early stopping if loss(epoch) doesn't improve for 10 epochs\n",
    "        patience, min_delta = 5, 0.0001\n",
    "        if epoch > patience:\n",
    "            loss_difference = q.np.mean(vae_model.losses['batch'][-patience:]) - vae_model.losses['batch'][-1]\n",
    "            if loss_difference < min_delta:\n",
    "                print('Early stopping - epoch')\n",
    "                break\n",
    "\n",
    "        # cuda empty cache\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x_test_ptc[:10], data.y_test_ptc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model(data.x_test_ptc[:10])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "vae_model\n",
    "vae_model.process_decoded_outputs(vae_model(data.x_test_ptc[:10])[0], one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(vae_model.loss_list_epoch, label='train')\n",
    "plt.plot(vae_model.loss_list_epoch_test, label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.to('cpu')\n",
    "data.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logvar = vae_model.encoder(data.x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstructed = vae_model.decoder(mu)\n",
    "x_reconstructed = VAE.process_decoded_outputs(x_reconstructed, one_hot=vae_model.one_hot)\n",
    "x_reconstructed_norm = torch.norm(x_reconstructed.float(), dim=-1)\n",
    "x_norm = torch.norm(data.x_test.float(), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x_norm, x_reconstructed_norm, alpha=0.5, s=10, color='orange')\n",
    "plt.plot([q.np.max(x_norm.numpy()), q.np.min(x_norm.numpy())], [q.np.max(x_norm.numpy()), q.np.min(x_norm.numpy())], color='black', linestyle='--')\n",
    "plt.xlabel('Norm of original vector')\n",
    "plt.ylabel('Norm of reconstructed vector')\n",
    "plt.title('Norm of original vector vs norm of reconstructed vector (no noise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstructed_noise = vae_model.decoder(vae_model.reparameterize(mu, logvar))\n",
    "x_reconstructed_noise = VAE.process_decoded_outputs(x_reconstructed_noise, one_hot=vae_model.one_hot)\n",
    "x_reconstructed_noise_norm = torch.norm(x_reconstructed_noise.float(), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x_norm, x_reconstructed_noise_norm, alpha=0.5, s=10, color='orange')\n",
    "plt.plot([q.np.max(x_norm.numpy()), q.np.min(x_norm.numpy())], [q.np.max(x_norm.numpy()), q.np.min(x_norm.numpy())], color='black', linestyle='--')\n",
    "plt.xlabel('Norm of original vector')\n",
    "plt.ylabel('Norm of reconstructed vector (with noise)')\n",
    "plt.title('Norm of original vector vs norm of reconstructed vector (with noise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_predicted = vae_model.score_predictor(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(data.y_test.tolist(), score_predicted.tolist(), alpha=0.5, s=10)\n",
    "plt.plot([q.np.max(data.y_test.numpy()), q.np.min(data.y_test.numpy())], [q.np.max((data.y_test).numpy()), q.np.min((data.y_test).numpy())], color='black', linestyle='--')\n",
    "plt.xlabel('Real score')\n",
    "plt.ylabel('Predicted score')\n",
    "plt.title('Predicted score vs real score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_predicted = vae_model.score_predictor(vae_model.reparameterize(mu, logvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(data.y_test.tolist(), score_predicted.tolist(), alpha=0.5, s=10)\n",
    "plt.plot([q.np.max(data.y_test.numpy()), q.np.min(data.y_test.numpy())], [q.np.max((data.y_test).numpy()), q.np.min((data.y_test).numpy())], color='black', linestyle='--')\n",
    "plt.xlabel('Real score')\n",
    "plt.ylabel('Predicted score')\n",
    "plt.title('Predicted score vs real score (with noise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find vector in laten space that gives the lowest score\n",
    "vector = torch.randn(1, 16)\n",
    "vector.requires_grad = True\n",
    "optimizer = optim.Adam([vector], lr=0.01)\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    score_predicted = vae_model.score_predictor(vector)\n",
    "    score_predicted.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {i+1}/1000, score={score_predicted.item():.6f}                                                       ', end='\\r')\n",
    "\n",
    "    # early stopping\n",
    "    patience = 20\n",
    "    min_delta = 0.01\n",
    "    if i > patience:\n",
    "        score_difference = q.np.mean(score_predicted.item() - score_predicted.item()) - score_predicted.item()\n",
    "        if score_difference < min_delta:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "print(vector, score_predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.requires_grad = False\n",
    "vae_model.eval()\n",
    "\n",
    "new_sequence = vae_model.decoder(vector)\n",
    "new_sequence = VAE.process_decoded_outputs(new_sequence, one_hot=vae_model.one_hot)\n",
    "\n",
    "score_new_sequence = vae_model.score_predictor(vae_model.encoder(new_sequence)[0])\n",
    "\n",
    "print(f'New sequence: {new_sequence.tolist()}, score={score_new_sequence.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_if_tensor_in_data(data, tensor):\n",
    "#     in_test = torch.any(torch.all(data.x_test == tensor, dim=-1))\n",
    "#     in_train = torch.any(torch.all(data.x_train == tensor, dim=-1))\n",
    "#     return in_test or in_train\n",
    "\n",
    "# def get_random_tensor(data, length=12, max_int=17):\n",
    "#     tensor = torch.randint(0, max_int+1, (length,))\n",
    "#     while check_if_tensor_in_data(data, tensor):\n",
    "#         tensor = torch.randint(0, max_int+1, (length,))\n",
    "#     return tensor\n",
    "\n",
    "# def get_random_tensors(data, n_tensors, length=12, max_int=17):\n",
    "#     tensors = []\n",
    "#     for i in range(n_tensors):\n",
    "#         print(i+1, '', end='\\r')\n",
    "#         tensors.append(get_random_tensor(data, length, max_int))\n",
    "#     return torch.stack(tensors)\n",
    "\n",
    "# data.to('cpu')\n",
    "# random_x = get_random_tensors(data, n_tensors=100000, length=12, max_int=17)\n",
    "# zeros_y = torch.zeros(len(random_x))\n",
    "\n",
    "# #change type of tensors of random_x and zeros_y to old_x.dtype and old_y.dtype\n",
    "# random_x = random_x.to(data.x_train.dtype)\n",
    "# zeros_y = zeros_y.to(data.y_train.dtype)\n",
    "\n",
    "# new_x = torch.cat((data.x_train.to('cpu'), random_x.to('cpu')), dim=0).to(device)\n",
    "# new_y = torch.cat((data.y_train.to('cpu'), zeros_y.to('cpu')), dim=0).to(device)\n",
    "\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# new_data_loader = DataLoader(TensorDataset(new_x, new_y), batch_size=data_loader.batch_size, shuffle=True)\n",
    "\n",
    "# data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstructed_test, mu_test, logvar_test = vae_model(data.x_test[:100])\n",
    "x_reconstructed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = VAE.loss_function(x_reconstructed_test, data.x_test[:100], mu_test, logvar_test, one_hot=vae_model.one_hot, reduction='mean')\n",
    "print(f'Loss on test set: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = VAE.process_decoded_outputs(x_reconstructed_test, one_hot=vae_model.one_hot)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (pred == data.x_test[:100]).float().mean().item()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, p in zip(data.x_test, pred):\n",
    "    print(x.tolist(), p.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, p in zip(data.x_test, x_reconstructed_test):\n",
    "    print(x.tolist(), p.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JL_Pennylane",
   "language": "python",
   "name": "jl_pennylane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
