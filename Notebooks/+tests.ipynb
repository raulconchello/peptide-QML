{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {} \n",
    "if x:\n",
    "    print('True')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.all([True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample tensor\n",
    "distance = torch.tensor([5.1, 2.4, 3.7, 0.9, 6.2])\n",
    "\n",
    "# Get the indices of the two smallest values\n",
    "values, indices = torch.topk(distance, k=2, largest=False)\n",
    "\n",
    "print(indices)  # tensor([3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the extracted values\n",
    "targets = []\n",
    "outputs = []\n",
    "losses = []\n",
    "\n",
    "# Splitting the content by lines and iterating through them\n",
    "with open('../checkpoints/0809/txts/0809-sh_em_1qm_v2_validation_1.txt', 'r') as file:\n",
    "    for line in file.read().split('\\n'):\n",
    "        if \"target:\" in line:\n",
    "            parts = line.split(',')\n",
    "            targets.append(float(parts[1].split(':')[1].strip()))\n",
    "            outputs.append(float(parts[2].split(':')[1].strip()))\n",
    "            losses.append(float(parts[3].split(':')[1].strip()))\n",
    "\n",
    "y_test, y_pred, losses = np.array(targets)*1000, np.array(outputs)*1000, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, color='r', label='Actual vs. Predicted', alpha=0.1)\n",
    "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], 'k--', lw=2, label='1:1 Line')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_stop_training(losses, lookback_epochs=5, threshold=0.001):\n",
    "    \"\"\"\n",
    "    Determines if training should stop based on the slope of the linear regression line fitted to the last `lookback_epochs` losses.\n",
    "    \n",
    "    Parameters:\n",
    "    - losses (list): List of loss values, where the most recent loss is the last element.\n",
    "    - lookback_epochs (int): Number of recent epochs to consider.\n",
    "    - threshold (float): Slope threshold to determine if training should stop.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if training should stop, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If there aren't enough epochs yet, continue training\n",
    "    if len(losses) < lookback_epochs:\n",
    "        return False\n",
    "    \n",
    "    # Extract the last `lookback_epochs` losses\n",
    "    recent_losses = np.array(losses[-lookback_epochs:])\n",
    "    epochs = np.array(range(1, lookback_epochs + 1))\n",
    "    \n",
    "    # Calculate the slope of the linear regression line using numpy's polyfit\n",
    "    slope, _ = np.polyfit(epochs, recent_losses, 1)\n",
    "    print(-slope)\n",
    "\n",
    "    # If the absolute slope is below the threshold, stop the training\n",
    "    return -slope < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [0.05, 0.09, 0.08, 0.05, 0.06, 0.05]\n",
    "\n",
    "print(should_stop_training(losses))\n",
    "\n",
    "#plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#object\n",
    "a = {'hello': 'world'}\n",
    "\n",
    "#save\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#load\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print(a == b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_dim = 10\n",
    "block_n_layers = 3\n",
    "n_neurons = []\n",
    "layers_dim = np.linspace(input_dim, input_dim**2, block_n_layers).astype(int)\n",
    "for n in zip(layers_dim[:-1], layers_dim[1:]): n_neurons.append(n)\n",
    "layers_dim = np.linspace(input_dim**2, 1, block_n_layers).astype(int)\n",
    "for n in zip(layers_dim[:-1], layers_dim[1:]): n_neurons.append(n)\n",
    "\n",
    "print(n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "print(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_id = str(uuid.uuid4())\n",
    "str_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    [\"uuid\", \"name\", \"description\"],\n",
    "    [\"1a2b3c4d\", \"Object 1\", \"Description for Object 1\"],\n",
    "    [\"5e6f7g8h\", \"Object 2\", \"Description for Object 2\"],\n",
    "]\n",
    "\n",
    "# Save data to a CSV file\n",
    "with open(\"objects.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    {\"uuid\": \"9i8u7y6t\", \"name\": \"Object 3\",   \"new_col\": \"New Column 1\", \"description\": \"Description, for Object 3\",},\n",
    "]\n",
    "\n",
    "with open('objects.csv', 'a', newline='') as csv_file:\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=new_data[0].keys())\n",
    "    csv_writer.writerows(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('objects.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "    # Loop through rows and display data\n",
    "    for row in csv_reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "\n",
    "def zip_folder(folder_path, zip_filename):\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, folder_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "# Example usage\n",
    "folder_to_zip = './peptide-QML/checkpoints/'\n",
    "zip_filename = 'output.zip'\n",
    "zip_folder(folder_to_zip, zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5,6,7,8,9,10]\n",
    "x.append(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append([12,13,14,15,16,17,18,19,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dict(iter({\n",
    "    'a': 1,\n",
    "    'b': 2,\n",
    "}.items()))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from itertools import product\n",
    "\n",
    "class sweep:\n",
    "    def __init__(self, name_notebook, **params):\n",
    "        self.name_notebook = name_notebook\n",
    "        self.uuid = uuid.uuid4()\n",
    "        self.params = params        \n",
    "        self.points = list(product(*params.values()))\n",
    "        self.n_points = len(self.points)\n",
    "\n",
    "        self.added_info = {k: {} for k in range(0, self.n_points)}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for index, point in enumerate(self.points):\n",
    "            yield {'idx': index, **dict(zip(self.params.keys(), point))}\n",
    "\n",
    "    def get_iter_with_info(self):\n",
    "        for index, point in enumerate(self.points):\n",
    "            yield {'idx': index, **dict(zip(self.params.keys(), point)), **self.added_info[index]}\n",
    "\n",
    "    def add_info(self, idx, **info):\n",
    "        self.added_info[idx].update(info)\n",
    "\n",
    "    def get_info(self, idx):\n",
    "        return self.added_info[idx]\n",
    "    \n",
    "    def save(self):\n",
    "        with open(f'{self.name_notebook}.pickle', 'wb') as handle:\n",
    "            pickle.dump(self, handle, )\n",
    "\n",
    "# Test\n",
    "s = sweep('test_notebook', a=[1,2,3], b=[4,2,1])\n",
    "s.add_info(1, loss=0.1, acc=0.9)\n",
    "for x in s.get_iter_with_info():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'a': 1, 'b': 2}\n",
    "x.update({'b': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define a toy dataset (randomly generated)\n",
    "num_samples = 1000\n",
    "seq_length = 20\n",
    "vocab_size = 1000\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # Number of classes for classification\n",
    "\n",
    "class MyEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(MyEmbeddingModel, self).__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Define additional layers (e.g., LSTM, fully connected layers, etc.)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Forward pass through the LSTM layer\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # Apply the fully connected layer to get the final output\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Use the last time step's output\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generate random input data\n",
    "X = torch.randint(0, vocab_size, (num_samples, seq_length)).int()\n",
    "y = torch.randint(0, 2, (num_samples,)).view(-1, 1).float()\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of MyEmbeddingModel\n",
    "model = MyEmbeddingModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print loss for monitoring training progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"acc\": 12, \"loss\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weights_shapes = [(6, 2)]\n",
    "weights_sizes = [np.prod(shape) for shape in weights_shapes]\n",
    "weights_size = np.sum(weights_sizes)\n",
    "\n",
    "def split_weights(weights):\n",
    "    w = []\n",
    "    for i in range(len(weights_sizes)):\n",
    "        w.append(weights[:weights_sizes[i]].reshape(weights_shapes[i]))\n",
    "        weights = weights[weights_sizes[i]:]\n",
    "    return w\n",
    "\n",
    "weights = np.random.randn(weights_size)\n",
    "x = split_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20680581,  1.70509248],\n",
       "       [-0.23903082, -0.75552226],\n",
       "       [ 0.77779355,  1.09406288],\n",
       "       [ 0.6119949 , -1.97877027],\n",
       "       [-0.85688343, -1.25664143],\n",
       "       [ 0.45412062,  0.22179532]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from my_code import helper_functions as f\n",
    "\n",
    "number_to_letter = {v:k for k,v in f.letter_to_number.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'G',\n",
       " 1: 'I',\n",
       " 2: 'L',\n",
       " 3: 'M',\n",
       " 4: 'F',\n",
       " 5: 'W',\n",
       " 6: 'Y',\n",
       " 7: 'V',\n",
       " 8: 'R',\n",
       " 9: 'K',\n",
       " 10: 'S',\n",
       " 11: 'T',\n",
       " 12: 'N',\n",
       " 13: 'Q',\n",
       " 14: 'H',\n",
       " 15: 'A',\n",
       " 16: 'C',\n",
       " 17: 'D',\n",
       " 18: 'E'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_to_letter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PennyLane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
