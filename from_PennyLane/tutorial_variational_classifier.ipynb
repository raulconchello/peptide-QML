{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n",
        "# It can be customized to whatever you like\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variational classifier {#variational_classifier}\n",
        "======================\n",
        "\n",
        "::: {.meta}\n",
        ":property=\\\"og:description\\\": Using PennyLane to implement quantum\n",
        "circuits that can be trained from labelled data to classify new data\n",
        "samples. :property=\\\"og:image\\\":\n",
        "<https://pennylane.ai/qml/_images/classifier_output_59_0.png>\n",
        ":::\n",
        "\n",
        "::: {.related}\n",
        "tutorial\\_data\\_reuploading\\_classifier Data-reuploading classifier\n",
        "tutorial\\_multiclass\\_classification Multiclass margin classifier\n",
        "ensemble\\_multi\\_qpu Ensemble classification with Rigetti and Qiskit\n",
        "devices\n",
        ":::\n",
        "\n",
        "*Author: Maria Schuld --- Posted: 11 October 2019. Last updated: 19\n",
        "January 2021.*\n",
        "\n",
        "In this tutorial, we show how to use PennyLane to implement variational\n",
        "quantum classifiers - quantum circuits that can be trained from labelled\n",
        "data to classify new data samples. The architecture is inspired by\n",
        "[Farhi and Neven (2018)](https://arxiv.org/abs/1802.06002) as well as\n",
        "[Schuld et al. (2018)](https://arxiv.org/abs/1804.00633).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will first show that the variational quantum classifier can reproduce\n",
        "the parity function\n",
        "\n",
        "$$\\begin{aligned}\n",
        "f: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y =\n",
        "\\begin{cases} 1 \\text{  if uneven number of ones in } x \\\\ 0\n",
        "\\text{ otherwise} \\end{cases}.\n",
        "\\end{aligned}$$\n",
        "\n",
        "This optimization example demonstrates how to encode binary inputs into\n",
        "the initial state of the variational circuit, which is simply a\n",
        "computational basis state.\n",
        "\n",
        "We then show how to encode real vectors as amplitude vectors (*amplitude\n",
        "encoding*) and train the model to recognize the first two classes of\n",
        "flowers in the Iris dataset.\n",
        "\n",
        "1. Fitting the parity function\n",
        "==============================\n",
        "\n",
        "Imports\n",
        "-------\n",
        "\n",
        "As before, we import PennyLane, the PennyLane-provided version of NumPy,\n",
        "and an optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.optimize import NesterovMomentumOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum and classical nodes\n",
        "===========================\n",
        "\n",
        "We create a quantum device with four \"wires\" (or qubits).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variational classifiers usually define a \"layer\" or \"block\", which is an\n",
        "elementary circuit architecture that gets repeated to build the\n",
        "variational circuit.\n",
        "\n",
        "Our circuit layer consists of an arbitrary rotation on every qubit, as\n",
        "well as CNOTs that entangle each qubit with its neighbour.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(W):\n",
        "\n",
        "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
        "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
        "    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n",
        "    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n",
        "\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.CNOT(wires=[1, 2])\n",
        "    qml.CNOT(wires=[2, 3])\n",
        "    qml.CNOT(wires=[3, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need a way to encode data inputs $x$ into the circuit, so that\n",
        "the measured output depends on the inputs. In this first example, the\n",
        "inputs are bitstrings, which we encode into the state of the qubits. The\n",
        "quantum state $\\psi$ after state preparation is a computational basis\n",
        "state that has 1s where $x$ has 1s, for example\n",
        "\n",
        "$$x = 0101 \\rightarrow |\\psi \\rangle = |0101 \\rangle .$$\n",
        "\n",
        "We use the `~pennylane.BasisState`{.interpreted-text role=\"class\"}\n",
        "function provided by PennyLane, which expects `x` to be a list of zeros\n",
        "and ones, i.e. `[0,1,0,1]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def statepreparation(x):\n",
        "    qml.BasisState(x, wires=[0, 1, 2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the quantum node as a state preparation routine, followed\n",
        "by a repetition of the layer structure. Borrowing from machine learning,\n",
        "we call the parameters `weights`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def circuit(weights, x):\n",
        "\n",
        "    statepreparation(x)\n",
        "\n",
        "    for W in weights:\n",
        "        layer(W)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different from previous examples, the quantum node takes the data as a\n",
        "keyword argument `x` (with the default value `None`). Keyword arguments\n",
        "of a quantum node are considered as fixed when calculating a gradient;\n",
        "they are never trained.\n",
        "\n",
        "If we want to add a \"classical\" bias parameter, the variational quantum\n",
        "classifier also needs some post-processing. We define the final model by\n",
        "a classical node that uses the first variable, and feeds the remainder\n",
        "into the quantum node. Before this, we reshape the list of remaining\n",
        "variables for easy use in the quantum node.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def variational_classifier(weights, bias, x):\n",
        "    return circuit(weights, x) + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cost\n",
        "====\n",
        "\n",
        "In supervised learning, the cost function is usually the sum of a loss\n",
        "function and a regularizer. We use the standard square loss that\n",
        "measures the distance between target labels and model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def square_loss(labels, predictions):\n",
        "    loss = 0\n",
        "    for l, p in zip(labels, predictions):\n",
        "        loss = loss + (l - p) ** 2\n",
        "\n",
        "    loss = loss / len(labels)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To monitor how many inputs the current classifier predicted correctly,\n",
        "we also define the accuracy given target labels and model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def accuracy(labels, predictions):\n",
        "\n",
        "    loss = 0\n",
        "    for l, p in zip(labels, predictions):\n",
        "        if abs(l - p) < 1e-5:\n",
        "            loss = loss + 1\n",
        "    loss = loss / len(labels)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For learning tasks, the cost depends on the data - here the features and\n",
        "labels considered in the iteration of the optimization routine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(weights, bias, X, Y):\n",
        "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
        "    return square_loss(Y, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n",
        "============\n",
        "\n",
        "Let's now load and preprocess some data.\n",
        "\n",
        "::: {.note}\n",
        "::: {.title}\n",
        "Note\n",
        ":::\n",
        "\n",
        "The parity dataset can be downloaded\n",
        "`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/demonstrations/variational_classifier/data/parity.txt\"\n",
        "download=parity.txt target=\"_blank\">here</a>`{.interpreted-text\n",
        "role=\"html\"} and should be placed in the subfolder\n",
        "`variational_classifier/data`.\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X = [0. 0. 0. 0.], Y = -1\n",
            "X = [0. 0. 0. 1.], Y =  1\n",
            "X = [0. 0. 1. 0.], Y =  1\n",
            "X = [0. 0. 1. 1.], Y = -1\n",
            "X = [0. 1. 0. 0.], Y =  1\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "data = np.loadtxt(\"../data/PennyLane/parity.txt\")\n",
        "X = np.array(data[:, :-1], requires_grad=False)\n",
        "Y = np.array(data[:, -1], requires_grad=False)\n",
        "Y = Y * 2 - np.ones(len(Y))  # shift label from {0, 1} to {-1, 1}\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"X = {}, Y = {: d}\".format(X[i], int(Y[i])))\n",
        "\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize the variables randomly (but fix a seed for\n",
        "reproducibility). The first variable in the list is used as a bias,\n",
        "while the rest is fed into the gates of the variational circuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "num_qubits = 4\n",
        "num_layers = 2\n",
        "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
        "bias_init = np.array(0.0, requires_grad=True)\n",
        "\n",
        "print(weights_init, bias_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we create an optimizer and choose a batch size...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = NesterovMomentumOptimizer(0.5)\n",
        "batch_size = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and train the optimizer. We track the accuracy - the share of\n",
        "correctly classified data samples. For this we compute the outputs of\n",
        "the variational classifier and turn them into predictions in $\\{-1,1\\}$\n",
        "by taking the sign of the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = weights_init\n",
        "bias = bias_init\n",
        "for it in range(25):\n",
        "\n",
        "    # Update the weights by one optimizer step\n",
        "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
        "    X_batch = X[batch_index]\n",
        "    Y_batch = Y[batch_index]\n",
        "    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
        "\n",
        "    # Compute accuracy\n",
        "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
        "    acc = accuracy(Y, predictions)\n",
        "\n",
        "    print(\n",
        "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
        "            it + 1, cost(weights, bias, X, Y), acc\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Iris classification\n",
        "======================\n",
        "\n",
        "Quantum and classical nodes\n",
        "---------------------------\n",
        "\n",
        "To encode real-valued vectors into the amplitudes of a quantum state, we\n",
        "use a 2-qubit simulator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "State preparation is not as simple as when we represent a bitstring with\n",
        "a basis state. Every input x has to be translated into a set of angles\n",
        "which can get fed into a small routine for state preparation. To\n",
        "simplify things a bit, we will work with data from the positive\n",
        "subspace, so that we can ignore signs (which would require another\n",
        "cascade of rotations around the z axis).\n",
        "\n",
        "The circuit is coded according to the scheme in [Möttönen, et al.\n",
        "(2004)](https://arxiv.org/abs/quant-ph/0407010), or---as presented for\n",
        "positive vectors only---in [Schuld and Petruccione\n",
        "(2018)](https://link.springer.com/book/10.1007/978-3-319-96424-9). We\n",
        "had to also decompose controlled Y-axis rotations into more basic\n",
        "circuits following [Nielsen and Chuang\n",
        "(2010)](http://www.michaelnielsen.org/qcqi/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_angles(x):\n",
        "\n",
        "    beta0 = 2 * np.arcsin(np.sqrt(x[1] ** 2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12))\n",
        "    beta1 = 2 * np.arcsin(np.sqrt(x[3] ** 2) / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12))\n",
        "    beta2 = 2 * np.arcsin(\n",
        "        np.sqrt(x[2] ** 2 + x[3] ** 2)\n",
        "        / np.sqrt(x[0] ** 2 + x[1] ** 2 + x[2] ** 2 + x[3] ** 2)\n",
        "    )\n",
        "\n",
        "    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])\n",
        "\n",
        "\n",
        "def statepreparation(a):\n",
        "    qml.RY(a[0], wires=0)\n",
        "\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.RY(a[1], wires=1)\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.RY(a[2], wires=1)\n",
        "\n",
        "    qml.PauliX(wires=0)\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.RY(a[3], wires=1)\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.RY(a[4], wires=1)\n",
        "    qml.PauliX(wires=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test if this routine actually works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array([0.53896774, 0.79503606, 0.27826503, 0.0], requires_grad=False)\n",
        "ang = get_angles(x)\n",
        "\n",
        "\n",
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def test(angles):\n",
        "\n",
        "    statepreparation(angles)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "\n",
        "test(ang)\n",
        "\n",
        "print(\"x               : \", x)\n",
        "print(\"angles          : \", ang)\n",
        "print(\"amplitude vector: \", np.real(dev.state))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the `default.qubit` simulator provides a shortcut to\n",
        "`statepreparation` with the command\n",
        "`qml.QubitStateVector(x, wires=[0, 1])`. However, some devices may not\n",
        "support an arbitrary state-preparation routine.\n",
        "\n",
        "Since we are working with only 2 qubits now, we need to update the layer\n",
        "function as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(W):\n",
        "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
        "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
        "    qml.CNOT(wires=[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variational classifier model and its cost remain essentially the\n",
        "same, but we have to reload them with the new state preparation and\n",
        "layer functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def circuit(weights, angles):\n",
        "    statepreparation(angles)\n",
        "\n",
        "    for W in weights:\n",
        "        layer(W)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "\n",
        "def variational_classifier(weights, bias, angles):\n",
        "    return circuit(weights, angles) + bias\n",
        "\n",
        "\n",
        "def cost(weights, bias, features, labels):\n",
        "    predictions = [variational_classifier(weights, bias, f) for f in features]\n",
        "    return square_loss(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data\n",
        "====\n",
        "\n",
        "We then load the Iris data set. There is a bit of preprocessing to do in\n",
        "order to encode the inputs into the amplitudes of a quantum state. In\n",
        "the last preprocessing step, we translate the inputs x to rotation\n",
        "angles using the `get_angles` function we defined above.\n",
        "\n",
        "::: {.note}\n",
        "::: {.title}\n",
        "Note\n",
        ":::\n",
        "\n",
        "The Iris dataset can be downloaded\n",
        "`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/demonstrations/variational_classifier/data/iris_classes1and2_scaled.txt\"\n",
        "download=parity.txt target=\"_blank\">here</a>`{.interpreted-text\n",
        "role=\"html\"} and should be placed in the subfolder\n",
        "`variational_classifer/data`.\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First X sample (original)  : [0.4  0.75]\n",
            "First X sample (padded)    : [0.4  0.75 0.3  0.  ]\n",
            "First X sample (normalized): [0.44376016 0.83205029 0.33282012 0.        ]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'get_angles' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst X sample (normalized):\u001b[39m\u001b[39m\"\u001b[39m, X_norm[\u001b[39m0\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[39m# angles for state preparation are new features\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([get_angles(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X_norm], requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst features sample      :\u001b[39m\u001b[39m\"\u001b[39m, features[\u001b[39m0\u001b[39m])\n\u001b[0;32m     19\u001b[0m Y \u001b[39m=\u001b[39m data[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
            "Cell \u001b[1;32mIn[13], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst X sample (normalized):\u001b[39m\u001b[39m\"\u001b[39m, X_norm[\u001b[39m0\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[39m# angles for state preparation are new features\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([get_angles(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X_norm], requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst features sample      :\u001b[39m\u001b[39m\"\u001b[39m, features[\u001b[39m0\u001b[39m])\n\u001b[0;32m     19\u001b[0m Y \u001b[39m=\u001b[39m data[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'get_angles' is not defined"
          ]
        }
      ],
      "source": [
        "data = np.loadtxt(\"../data/PennyLane/iris_classes1and2_scaled.txt\")\n",
        "X = data[:, 0:2]\n",
        "print(\"First X sample (original)  :\", X[0])\n",
        "\n",
        "# pad the vectors to size 2^2 with constant values\n",
        "padding = 0.3 * np.ones((len(X), 1))\n",
        "X_pad = np.c_[np.c_[X, padding], np.zeros((len(X), 1))]\n",
        "print(\"First X sample (padded)    :\", X_pad[0])\n",
        "\n",
        "# normalize each input\n",
        "normalization = np.sqrt(np.sum(X_pad ** 2, -1))\n",
        "X_norm = (X_pad.T / normalization).T\n",
        "print(\"First X sample (normalized):\", X_norm[0])\n",
        "\n",
        "# angles for state preparation are new features\n",
        "features = np.array([get_angles(x) for x in X_norm], requires_grad=False)\n",
        "print(\"First features sample      :\", features[0])\n",
        "\n",
        "Y = data[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These angles are our new features, which is why we have renamed X to\n",
        "\"features\" above. Let's plot the stages of preprocessing and play around\n",
        "with the dimensions (dim1, dim2). Some of them still separate the\n",
        "classes well, while others are less informative.\n",
        "\n",
        "*Note: To run the following code you need the matplotlib library.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"b\", marker=\"o\", edgecolors=\"k\")\n",
        "plt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"r\", marker=\"o\", edgecolors=\"k\")\n",
        "plt.title(\"Original data\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "dim1 = 0\n",
        "dim2 = 1\n",
        "plt.scatter(\n",
        "    X_norm[:, dim1][Y == 1], X_norm[:, dim2][Y == 1], c=\"b\", marker=\"o\", edgecolors=\"k\"\n",
        ")\n",
        "plt.scatter(\n",
        "    X_norm[:, dim1][Y == -1], X_norm[:, dim2][Y == -1], c=\"r\", marker=\"o\", edgecolors=\"k\"\n",
        ")\n",
        "plt.title(\"Padded and normalised data (dims {} and {})\".format(dim1, dim2))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "dim1 = 0\n",
        "dim2 = 3\n",
        "plt.scatter(\n",
        "    features[:, dim1][Y == 1], features[:, dim2][Y == 1], c=\"b\", marker=\"o\", edgecolors=\"k\"\n",
        ")\n",
        "plt.scatter(\n",
        "    features[:, dim1][Y == -1], features[:, dim2][Y == -1], c=\"r\", marker=\"o\", edgecolors=\"k\"\n",
        ")\n",
        "plt.title(\"Feature vectors (dims {} and {})\".format(dim1, dim2))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time we want to generalize from the data samples. To monitor the\n",
        "generalization performance, the data is split into training and\n",
        "validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "num_data = len(Y)\n",
        "num_train = int(0.75 * num_data)\n",
        "index = np.random.permutation(range(num_data))\n",
        "feats_train = features[index[:num_train]]\n",
        "Y_train = Y[index[:num_train]]\n",
        "feats_val = features[index[num_train:]]\n",
        "Y_val = Y[index[num_train:]]\n",
        "\n",
        "# We need these later for plotting\n",
        "X_train = X[index[:num_train]]\n",
        "X_val = X[index[num_train:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n",
        "============\n",
        "\n",
        "First we initialize the variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_qubits = 2\n",
        "num_layers = 6\n",
        "\n",
        "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
        "bias_init = np.array(0.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we optimize the cost. This may take a little patience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = NesterovMomentumOptimizer(0.01)\n",
        "batch_size = 5\n",
        "\n",
        "# train the variational classifier\n",
        "weights = weights_init\n",
        "bias = bias_init\n",
        "for it in range(60):\n",
        "\n",
        "    # Update the weights by one optimizer step\n",
        "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
        "    feats_train_batch = feats_train[batch_index]\n",
        "    Y_train_batch = Y_train[batch_index]\n",
        "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
        "\n",
        "    # Compute predictions on train and validation set\n",
        "    predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in feats_train]\n",
        "    predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in feats_val]\n",
        "\n",
        "    # Compute accuracy on train and validation set\n",
        "    acc_train = accuracy(Y_train, predictions_train)\n",
        "    acc_val = accuracy(Y_val, predictions_val)\n",
        "\n",
        "    print(\n",
        "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
        "        \"\".format(it + 1, cost(weights, bias, features, Y), acc_train, acc_val)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the continuous output of the variational classifier for the\n",
        "first two dimensions of the Iris data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "cm = plt.cm.RdBu\n",
        "\n",
        "# make data for decision regions\n",
        "xx, yy = np.meshgrid(np.linspace(0.0, 1.5, 20), np.linspace(0.0, 1.5, 20))\n",
        "X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n",
        "\n",
        "# preprocess grid points like data inputs above\n",
        "padding = 0.3 * np.ones((len(X_grid), 1))\n",
        "X_grid = np.c_[np.c_[X_grid, padding], np.zeros((len(X_grid), 1))]  # pad each input\n",
        "normalization = np.sqrt(np.sum(X_grid ** 2, -1))\n",
        "X_grid = (X_grid.T / normalization).T  # normalize each input\n",
        "features_grid = np.array(\n",
        "    [get_angles(x) for x in X_grid]\n",
        ")  # angles for state preparation are new features\n",
        "predictions_grid = [variational_classifier(weights, bias, f) for f in features_grid]\n",
        "Z = np.reshape(predictions_grid, xx.shape)\n",
        "\n",
        "# plot decision regions\n",
        "cnt = plt.contourf(\n",
        "    xx, yy, Z, levels=np.arange(-1, 1.1, 0.1), cmap=cm, alpha=0.8, extend=\"both\"\n",
        ")\n",
        "plt.contour(\n",
        "    xx, yy, Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,)\n",
        ")\n",
        "plt.colorbar(cnt, ticks=[-1, 0, 1])\n",
        "\n",
        "# plot data\n",
        "plt.scatter(\n",
        "    X_train[:, 0][Y_train == 1],\n",
        "    X_train[:, 1][Y_train == 1],\n",
        "    c=\"b\",\n",
        "    marker=\"o\",\n",
        "    edgecolors=\"k\",\n",
        "    label=\"class 1 train\",\n",
        ")\n",
        "plt.scatter(\n",
        "    X_val[:, 0][Y_val == 1],\n",
        "    X_val[:, 1][Y_val == 1],\n",
        "    c=\"b\",\n",
        "    marker=\"^\",\n",
        "    edgecolors=\"k\",\n",
        "    label=\"class 1 validation\",\n",
        ")\n",
        "plt.scatter(\n",
        "    X_train[:, 0][Y_train == -1],\n",
        "    X_train[:, 1][Y_train == -1],\n",
        "    c=\"r\",\n",
        "    marker=\"o\",\n",
        "    edgecolors=\"k\",\n",
        "    label=\"class -1 train\",\n",
        ")\n",
        "plt.scatter(\n",
        "    X_val[:, 0][Y_val == -1],\n",
        "    X_val[:, 1][Y_val == -1],\n",
        "    c=\"r\",\n",
        "    marker=\"^\",\n",
        "    edgecolors=\"k\",\n",
        "    label=\"class -1 validation\",\n",
        ")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "About the author\n",
        "================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
